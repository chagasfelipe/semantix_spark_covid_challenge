{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /SEMANTIX_ACADEMY \n",
    "\n",
    "### /PROJETO_FINAL_SPARK // NÍVEL BÁSICO // CAMPANHA NACIONAL DE VACINAÇÃO CONTRA CONVID-19\n",
    "\n",
    "    /ALUNO: FELIPE CHAGAS DE SOUZA\n",
    "    /E-MAIL: souza.chagas.felipe@gmail.com\n",
    "    /TREINAMENTO: BIG DATA ENGINNER // 2022\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:Imports e setup do logger realizados!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import logging\n",
    "import rarfile\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('LOGGER')\n",
    "logger.info('Imports e setup do logger realizados!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFININDO FUNÇÕES PARA DOWNLOAD E EXTRAÇÃO DOS ARQUIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:Funções para download e extração dos arquivos csv criadas com sucesso\n",
      "INFO:LOGGER:Para utilizar o extrator de RAR files: extract_rar_files(diretorio_destino, diretorio_origem, url_arquivo)\n"
     ]
    }
   ],
   "source": [
    "def download_file(url):\n",
    "    logger.info('Realizando download do arquivo da origem saude.gov.br')\n",
    "    url = url\n",
    "\n",
    "    request = requests.get(url)\n",
    "    \n",
    "    return request\n",
    "\n",
    "def get_file_name(url):\n",
    "    file_name = url.split('/')[-1]\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "\n",
    "def extract_rar_files(diretorio_destino, diretorio_origem, url_arquivo):\n",
    "    url = download_file(url_arquivo)\n",
    "    logger.info('Download dos arquivos da origem saude.gov.br realizado')\n",
    "    file_name = get_file_name(url_arquivo)\n",
    "\n",
    "\n",
    "    diretorio_destino = diretorio_destino\n",
    "    arquivo_diretorio_origem = diretorio_origem + file_name\n",
    "    arquivo_extracao = diretorio_destino + file_name\n",
    "\n",
    "    if (os.path.exists(arquivo_extracao)) != True:\n",
    "\n",
    "        with open(file_name,'wb') as output_file:\n",
    "            output_file.write(request.content)\n",
    "\n",
    "        shutil.move(arquivo_diretorio_origem, diretorio_destino)\n",
    "\n",
    "    rf = rarfile.RarFile(arquivo_extracao, \"r\")\n",
    "    rf.extractall(diretorio_destino)\n",
    "    rf.close()\n",
    "\n",
    "    logger.info('Extração completa, arquivos movidos e extraídos para a pasta ' + diretorio_destino)\n",
    "    \n",
    "logger.info(\"Funções para download e extração dos arquivos csv criadas com sucesso\")\n",
    "logger.info(\"Para utilizar o extrator de RAR files: extract_rar_files(diretorio_destino, diretorio_origem, url_arquivo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOWNLOAD E EXTRAÇÃO DOS ARQUIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:Realizando download do arquivo da origem saude.gov.br\n",
      "INFO:LOGGER:Download dos arquivos da origem saude.gov.br realizado\n",
      "INFO:LOGGER:Extração completa, arquivos movidos e extraídos para a pasta /mnt/notebooks/data/\n"
     ]
    }
   ],
   "source": [
    "extract_rar_files('/mnt/notebooks/data/','/mnt/notebooks/project/', 'https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Enviar os dados para o hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   2 root supergroup   62492959 2022-04-21 01:33 /user/felipe/desafio_spark_data_raw/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup   76520681 2022-04-21 01:33 /user/felipe/desafio_spark_data_raw/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup   91120916 2022-04-21 01:33 /user/felipe/desafio_spark_data_raw/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup    3046774 2022-04-21 01:33 /user/felipe/desafio_spark_data_raw/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:Arquivos Painel Covidbr da origem saude.gov.br copiados para o HDFS\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/felipe/desafio_spark_data_raw\n",
    "!hdfs dfs -put /mnt/notebooks/data/*.csv /user/felipe/desafio_spark_data_raw\n",
    "!hdfs dfs -ls /user/felipe/desafio_spark_data_raw/*.csv\n",
    "\n",
    "logger.info('Arquivos Painel Covidbr da origem saude.gov.br copiados para o HDFS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por município."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    EXPLORAÇÃO E AJUSTE DO SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:Dataframe df_covid_brasil_base criado com sucesso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: string (nullable = true)\n",
      " |-- codmun: string (nullable = true)\n",
      " |-- codRegiaoSaude: string (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      " |-- semanaEpi: string (nullable = true)\n",
      " |-- populacaoTCU2019: string (nullable = true)\n",
      " |-- casosAcumulado: string (nullable = true)\n",
      " |-- casosNovos: string (nullable = true)\n",
      " |-- obitosAcumulado: string (nullable = true)\n",
      " |-- obitosNovos: string (nullable = true)\n",
      " |-- Recuperadosnovos: string (nullable = true)\n",
      " |-- emAcompanhamentoNovos: string (nullable = true)\n",
      " |-- interior/metropolitana: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Criação do df base\n",
    "df_covid_brasil_base = spark.read.csv(\"/user/felipe/desafio_spark_data_raw/*.csv\", sep=';'\\\n",
    "                                      , header='true', mode='DROPMALFORMED')\n",
    "\n",
    "df_covid_brasil_base.printSchema()\n",
    "logger.info(\"Dataframe df_covid_brasil_base criado com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:Dataframe df_covid_brasil criado com sucesso com schema alterado, baseado no df_covid_brasil_base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: integer (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#criando o df com o schema transformado\n",
    "df_covid_brasil = df_covid_brasil_base.withColumn(\"coduf\", col(\"coduf\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"codmun\", col(\"codmun\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"codRegiaoSaude\", col(\"codRegiaoSaude\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"data\", col(\"data\").cast(TimestampType()))\\\n",
    "                                        .withColumn(\"semanaEpi\", col(\"semanaEpi\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"populacaoTCU2019\", col(\"populacaoTCU2019\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"casosAcumulado\", col(\"casosAcumulado\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"casosNovos\", col(\"casosNovos\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"obitosAcumulado\", col(\"obitosAcumulado\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"obitosNovos\", col(\"obitosNovos\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"Recuperadosnovos\", col(\"Recuperadosnovos\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"emAcompanhamentoNovos\", col(\"emAcompanhamentoNovos\").cast(IntegerType()))\\\n",
    "                                        .withColumn(\"interior/metropolitana\", col(\"interior/metropolitana\").cast(IntegerType()))\n",
    "\n",
    "df_covid_brasil.printSchema()\n",
    "logger.info(\"Dataframe df_covid_brasil criado com sucesso com schema alterado, baseado no df_covid_brasil_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Registros: 2624943\n",
      "Total registros com coluna municipio nula: 23753\n"
     ]
    }
   ],
   "source": [
    "#Check Data\n",
    "#quantidade de dados no df\n",
    "print(\"Total Registros: {}\".format(df_covid_brasil.count()))\n",
    "\n",
    "#checando quantidade de dados com municipio nulo\n",
    "print(\"Total registros com coluna municipio nula: {}\".format(df_covid_brasil.filter(df_covid_brasil.municipio.isNull()).count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    LIMPEZA DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:Dataframe df_covid_brasil_fillna criado com sucesso, colunas do tipo int com valor nulo, recebe valor 0\n",
      "INFO:LOGGER:Dataframe df_covid_brasil_cleaned criado com sucesso, espaços no início e fim em colunas String foram removidos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: integer (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aplicando 0 para as colunas numéricas que estão nulas\n",
    "df_covid_brasil_fillna = df_covid_brasil.fillna(value=0, subset=[\"coduf\",\"codmun\",\\\n",
    "                                                                   \"codRegiaoSaude\", \"semanaEpi\",\\\n",
    "                                                                   \"populacaoTCU2019\", \"casosAcumulado\",\\\n",
    "                                                                   \"casosNovos\", \"obitosAcumulado\",\\\n",
    "                                                                   \"obitosNovos\", \"Recuperadosnovos\",\\\n",
    "                                                                   \"emAcompanhamentoNovos\", \"interior/metropolitana\"])\n",
    "\n",
    "logger.info(\"Dataframe df_covid_brasil_fillna criado com sucesso, colunas do tipo int com valor nulo, recebe valor 0\")\n",
    "\n",
    "#Eliminando espaços no começo e fim das colunas do tipo string\n",
    "df_covid_brasil_cleaned = df_covid_brasil_fillna.withColumn(\"regiao\", rtrim(ltrim(lower(col(\"regiao\")))))\\\n",
    "                                        .withColumn(\"estado\", rtrim(ltrim(lower(col(\"estado\")))))\\\n",
    "                                        .withColumn(\"municipio\", rtrim(ltrim(lower(col(\"municipio\")))))\\\n",
    "                                        .withColumn(\"nomeRegiaoSaude\", rtrim(ltrim(lower(col(\"nomeRegiaoSaude\")))))\n",
    "\n",
    "logger.info(\"Dataframe df_covid_brasil_cleaned criado com sucesso, espaços no início e fim em colunas String foram removidos\")\n",
    "df_covid_brasil_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    INGERINDO OS DADOS NO APACHE HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:A tabela tb_casos_covid foi criada com sucesso no Hive\n"
     ]
    }
   ],
   "source": [
    "df_covid_brasil_cleaned.cache()\n",
    "df_covid_brasil_cleaned.write.saveAsTable(\"tb_casos_covid\", mode=\"overwrite\", partitionBy=\"municipio\")\n",
    "\n",
    "logger.info(\"A tabela tb_casos_covid foi criada com sucesso no Hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:Validando criação Tabela tb_casos_covid no Hive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------+\n",
      "|  regiao|estado|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|               data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|municipio|\n",
      "+--------+------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------+\n",
      "|  brasil|  null|   76|     0|             0|           null|2021-01-01 00:00:00|       53|       210147125|       7700578|     24605|         195411|        462|         6756284|               748883|                     0|     null|\n",
      "|nordeste|    pi|   22|220000|             0|           null|2021-01-01 00:00:00|       53|               0|             0|         0|              0|          0|               0|                    0|                     0|     null|\n",
      "|  brasil|  null|   76|     0|             0|           null|2021-01-02 00:00:00|       53|       210147125|       7716405|     15827|         195725|        314|         6769420|               751260|                     0|     null|\n",
      "|nordeste|    pi|   22|220000|             0|           null|2021-01-02 00:00:00|       53|               0|             0|         0|              0|          0|               0|                    0|                     0|     null|\n",
      "+--------+------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------+\n",
      "only showing top 4 rows\n",
      "\n",
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: integer (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: integer (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      "\n",
      "Found 5299 items\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-21 03:53 /user/hive/warehouse/tb_casos_covid/_SUCCESS\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-21 03:51 /user/hive/warehouse/tb_casos_covid/municipio=__HIVE_DEFAULT_PARTITION__\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-21 03:51 /user/hive/warehouse/tb_casos_covid/municipio=abadia de goiás\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Validando criação Tabela tb_casos_covid no Hive\")\n",
    "\n",
    "tb_casos_covid = spark.read.table(\"tb_casos_covid\")\n",
    "tb_casos_covid.show(4)\n",
    "tb_casos_covid.printSchema()\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse/tb_casos_covid | head -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS:\n",
    "\n",
    "![](https://raw.githubusercontent.com/chagasfelipe/semantix_spark_covid_challenge/main/img/painel_covid_escopo.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/CASOS RECUPERADOS\n",
      "+----------------+-----------------+\n",
      "|casos_ecuperados|em_acompanhamento|\n",
      "+----------------+-----------------+\n",
      "|      2920055795|        320793426|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "painel_casos_recuperados = tb_casos_covid.agg(sum(\"Recuperadosnovos\").alias(\"casos_ecuperados\")\\\n",
    "                                              , sum(\"emAcompanhamentoNovos\").alias(\"em_acompanhamento\"))\n",
    "\n",
    "print(\"/CASOS RECUPERADOS\")\n",
    "painel_casos_recuperados.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/CASOS CONFIRMADOS\n",
      "+----------+-----------+----------+\n",
      "| acumulado|casos_novos|incidencia|\n",
      "+----------+-----------+----------+\n",
      "|9998172092|   56565045|    3247.6|\n",
      "+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confirmados_base = tb_casos_covid.agg(sum(\"casosAcumulado\").alias(\"acumulado\")\\\n",
    "                                      , sum(\"casosNovos\").alias(\"casos_novos\")\n",
    "                                      , sum(\"populacaoTCU2019\").alias(\"populacao\"))\n",
    "\n",
    "painel_casos_confirmados = confirmados_base.withColumn(\"incidencia\", regexp_replace(format_number(\\\n",
    "                                        ((col('acumulado') * 100000) / col('populacao')), 1), \",\", \"\")).drop('populacao')\n",
    "\n",
    "print(\"/CASOS CONFIRMADOS\")\n",
    "painel_casos_confirmados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ÓBITOS CONFIRMADOS\n",
      "+---------+-----------+----------+-----------+\n",
      "|acumulado|casos_novos|letalidade|mortalidade|\n",
      "+---------+-----------+----------+-----------+\n",
      "|274784085|    1580676|      2.75|      89.25|\n",
      "+---------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obitos_base = tb_casos_covid.agg(sum(\"casosAcumulado\").alias(\"casos_acumulados\")\\\n",
    "                                 , sum(\"obitosAcumulado\").alias(\"acumulado\")\\\n",
    "                                 , sum(\"obitosNovos\").alias(\"casos_novos\")\\\n",
    "                                 , sum(\"populacaoTCU2019\").alias(\"populacao\"))\n",
    "\n",
    "painel_obitos_confirmados = obitos_base.withColumn(\"letalidade\"\\\n",
    "                            , format_number(((col('acumulado') * 100) / col('casos_acumulados')), 2))\\\n",
    "                            .withColumn(\"mortalidade\", format_number(((col('Acumulado') * 100000) / col('populacao')), 2))\\\n",
    "                            .drop(\"casos_acumulados\",\"populacao\")\n",
    "\n",
    "\n",
    "print(\"/ÓBITOS CONFIRMADOS\")\n",
    "painel_obitos_confirmados.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Salvar a primeira visualização como tabela Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-21 16:34 /user/hive/warehouse/tb_casos_recuperados_covid/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        748 2022-04-21 16:34 /user/hive/warehouse/tb_casos_recuperados_covid/part-00000-5acd4f20-f15c-40f7-9cad-bda449908ac3-c000.snappy.parquet\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:A visualização painel_casos_recuperados foi criada como tabela com sucesso no Hive\n"
     ]
    }
   ],
   "source": [
    "painel_casos_recuperados.write.saveAsTable(\"tb_casos_recuperados_covid\", mode=\"overwrite\", header=\"true\")\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse/tb_casos_recuperados_covid | head -5\n",
    "\n",
    "logger.info(\"A visualização painel_casos_recuperados foi criada como tabela com sucesso no Hive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Salvar a segunda visualização com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-21 16:35 /user/felipe/desafio_spark_data_raw/casos_confirmados/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        954 2022-04-21 16:35 /user/felipe/desafio_spark_data_raw/casos_confirmados/part-00000-c47a583a-d155-4cf9-b014-1a279ca04021-c000.snappy.parquet\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:A visualização painel_casos_confirmados foi criada como tabela com sucesso no Hive\n"
     ]
    }
   ],
   "source": [
    "painel_casos_confirmados.write.parquet(\"/user/felipe/desafio_spark_data_raw/casos_confirmados\", compression=\"snappy\")\n",
    "\n",
    "!hdfs dfs -ls /user/felipe/desafio_spark_data_raw/casos_confirmados | head -5\n",
    "\n",
    "logger.info(\"A visualização painel_casos_confirmados foi criada como tabela com sucesso no Hive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Salvar a terceira visualização em um tópico no Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:A visualização view_obitos_confirmados foi criada com sucesso\n"
     ]
    }
   ],
   "source": [
    "view_obitos_confirmados = painel_obitos_confirmados.select(\"acumulado\",\"casos_novos\",\"mortalidade\")\\\n",
    "                                       .withColumnRenamed(\"mortalidade\",\"value\")\\\n",
    "                                       .withColumn(\"value\",col(\"value\").cast(StringType()))\n",
    "logger.info(\"A visualização view_obitos_confirmados foi criada com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:A visualização painel_obitos_confirmados foi criada em um tópico kafka com sucesso!\n"
     ]
    }
   ],
   "source": [
    "view_obitos_confirmados.write.format('kafka')\\\n",
    "              .option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    "              .option(\"topic\",\"topic_obitos_confirmados\")\\\n",
    "              .save()\n",
    "\n",
    "logger.info(\"A visualização painel_obitos_confirmados foi criada em um tópico kafka com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No terminal do container do kafka\n",
    "##### docker exec -it kafka bash\n",
    "##### kafka-topics.sh --bootstrap-server kafka:9092 --list\n",
    "\n",
    "![](https://raw.githubusercontent.com/chagasfelipe/semantix_spark_covid_challenge/main/img/kafka_evidencia_criacao_topico.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Criar a visualização pelo Spark com os dados enviados para o HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/CASOS_COVID_BR\n",
      "+------------+----------+-------+----------+-----------+----------------+\n",
      "|      regiao|     Casos| Óbitos|Incidência|Mortalidade|     Atualização|\n",
      "+------------+----------+-------+----------+-----------+----------------+\n",
      "|      brasil|18,855,015|526,892|   8,972.3|      250.7|2021-07-06 00:00|\n",
      "|centro-oeste| 3,833,238| 98,414|  11,760.5|      301.9|2021-07-06 00:00|\n",
      "|    nordeste| 8,911,474|215,648|   7,807.3|      188.9|2021-07-06 00:00|\n",
      "|       norte| 3,465,630| 87,690|   9,401.6|      237.9|2021-07-06 00:00|\n",
      "|     sudeste|14,277,606|490,622|   8,078.2|      277.6|2021-07-06 00:00|\n",
      "|         sul| 7,222,082|161,410|  12,046.4|      269.2|2021-07-06 00:00|\n",
      "+------------+----------+-------+----------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cases_view = tb_casos_covid.filter(col(\"data\") == \"2021-07-06\")\n",
    "\n",
    "casos_visualizacao = base_cases_view.groupBy(\"regiao\", \"data\")\\\n",
    "    .agg(format_number(sum(\"casosAcumulado\"), 0).alias(\"Casos\")\\\n",
    "    , format_number(sum(\"obitosAcumulado\"), 0).alias(\"Óbitos\")\\\n",
    "    , format_number(sum(\"populacaoTCU2019\"), 0).alias(\"População\")\\\n",
    "    , format_number((sum(\"casosAcumulado\")/(sum(\"populacaoTCU2019\") / 100000)), 1).alias(\"Incidência\")\\\n",
    "    , format_number((sum(\"obitosAcumulado\")/(sum(\"populacaoTCU2019\") / 100000)), 1).alias(\"Mortalidade\"))\\\n",
    "    .sort(asc(\"regiao\"))\n",
    "\n",
    "print(\"/CASOS_COVID_BR\")\n",
    "casos_visualizacao.select(col(\"regiao\")\\\n",
    "                          , col(\"Casos\")\\\n",
    "                          , col(\"Óbitos\")\\\n",
    "                          , col(\"Incidência\")\\\n",
    "                          , col(\"Mortalidade\")\\\n",
    "                          , date_format(col(\"data\"), \"yyyy-MM-dd HH:mm\").alias(\"Atualização\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Salvar a visualização do exercício 6 em um tópico no Elastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NÃO CONSEGUI ENCONTRAR A BIBLIOTECA CORRETA PARA REALIZAR A CONEXÃO COM O ELASTIC COM O CÓDIGO ABAIXO:\n",
    "``` python\n",
    "view_obitos_confirmados.write.format(\"org.elasticsearch.spark.sql\")\\\n",
    "                                .option(\"es.index.auto.create\", \"true\")\\\n",
    "                                .option(\"es.nodes\", \"localhost\")\\\n",
    "                                .option(\"es.port\", \"9243\")\\\n",
    "                                .option(\"es.resource\", \"covid/elastic_obitos_confirmados\")\\\n",
    "                                .option(\"es.nodes.wan.only\", \"true\")\\\n",
    "                                .save()\n",
    "```\n",
    "\n",
    "##### Tentei com o seguintes jars:\n",
    "    https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-hadoop/7.13.3/elasticsearch-hadoop-7.13.3.jar\n",
    "    https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-hadoop/2.4.1/elasticsearch-hadoop-2.4.1.jar\n",
    "    https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-20_2.11/7.9.2/elasticsearch-spark-20_2.11-7.9.2.jar\n",
    "    https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-hadoop/7.9.2/elasticsearch-hadoop-7.9.2.jar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-21 15:25 /user/felipe/desafio_spark_data_raw/elastic/obitos_confirmados/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup         52 2022-04-21 15:25 /user/felipe/desafio_spark_data_raw/elastic/obitos_confirmados/part-00000-f887e8c9-8717-44a9-aef0-61d47cc0113b-c000.csv\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:A visualização view_obitos_confirmados foi criada em CSV com sucesso!\n"
     ]
    }
   ],
   "source": [
    "#COMO NÃO CONSEGUI ENCONTRAR A BIBLIOTECA CORRETA E REALIZAR A GRAVAÇÃO DA VIEW EM UM TÓPICO DO ELASTIC, \n",
    "#FIZ A GERAÇÃO DA VIEW EM CSV E REALIZEI A IMPORTAÇÃO MANUAL DOS DADOS ATRAVÉS DO KIBANA: MACHINE LEARNING > DATA ANALYZER\n",
    "\n",
    "view_obitos_confirmados.write.csv(\"/user/felipe/desafio_spark_data_raw/elastic/obitos_confirmados\", mode=\"overwrite\"\\\n",
    "                                  , sep=\",\", header=\"true\")\n",
    "!hdfs dfs -ls /user/felipe/desafio_spark_data_raw/elastic/obitos_confirmados/\n",
    "\n",
    "logger.info(\"A visualização view_obitos_confirmados foi criada em CSV com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LOGGER:CSV disponível em /mnt/notebooks/data/\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -get /user/felipe/desafio_spark_data_raw/elastic/obitos_confirmados/part-00000-f887e8c9-8717-44a9-aef0-61d47cc0113b-c000.csv /mnt/notebooks/data/\n",
    "\n",
    "logger.info(\"CSV disponível em /mnt/notebooks/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Criar um dashboard no Elastic para visualização dos novos dados enviados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### /EVIDÊNCIA_1_IMPORT_CSV\n",
    "![](https://raw.githubusercontent.com/chagasfelipe/semantix_spark_covid_challenge/main/img/elastic_evidencia_import.PNG)\n",
    "\n",
    "#### /EVIDÊNCIA_2_CRIACAO_INDICE\n",
    "![](https://raw.githubusercontent.com/chagasfelipe/semantix_spark_covid_challenge/main/img/elastic_evidencia_criacao_indice.PNG)\n",
    "\n",
    "#### /EVIDÊNCIA_3_DASHBOARD\n",
    "![](https://raw.githubusercontent.com/chagasfelipe/semantix_spark_covid_challenge/img/files/elastic_evidencia_dash.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
